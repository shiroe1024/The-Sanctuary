import requests
import json
import logging
from openai import OpenAI
import streamlit as st
import os

# --- CONFIG ---
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Initialize OpenAI
try:
    api_key = st.secrets.get("OPENAI_API_KEY", os.getenv("OPENAI_API_KEY"))
    client = OpenAI(api_key=api_key)
except Exception:
    client = None

# --- CONSTANTS ---
THE_ATLAS = {
    "Formal Sciences": ["Logic", "Mathematics", "Computer Science"],
    "Natural Sciences": ["Physics", "Chemistry", "Biology"],
    "Social Sciences": ["Economics", "Psychology", "Sociology"],
    "Humanities": ["Philosophy", "History", "Literature"],
    "Applied Sciences": ["Engineering", "Medicine", "Business"]
}

# --- THE HYDRA: Multiple Piped Mirrors ---
# If one fails, we move to the next.
PIPED_INSTANCES = [
    "https://pipedapi.kavin.rocks",
    "https://api.piped.io",
    "https://pipedapi.adminforge.de",
    "https://api.piped.privacy.com.de"
]

# --- THE DISGUISE: Fake Browser Headers ---
# This prevents the "403 Forbidden" error.
HEADERS = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36"
}

def get_transcript(video_id: str):
    """
    Tries multiple Piped instances to fetch English subtitles.
    """
    for base_url in PIPED_INSTANCES:
        try:
            # 1. Fetch Metadata
            api_url = f"{base_url}/streams/{video_id}"
            response = requests.get(api_url, headers=HEADERS, timeout=5)
            
            if response.status_code != 200:
                continue # Try next server
                
            data = response.json()
            
            # 2. Find English Subtitle Track
            subtitles = data.get('subtitles', [])
            subtitle_url = None
            
            for sub in subtitles:
                if sub['code'] == 'en':
                    subtitle_url = sub['url']
                    if not sub['autoGenerated']:
                        break # Prefer manual
            
            if not subtitle_url:
                continue # Try next server
                
            # 3. Fetch Text Content
            sub_resp = requests.get(subtitle_url, headers=HEADERS, timeout=5)
            if sub_resp.status_code == 200:
                return clean_vtt(sub_resp.text)
                
        except Exception as e:
            logger.warning(f"Failed on {base_url}: {e}")
            continue

    return None

def clean_vtt(raw_text):
    """
    Removes timestamps from VTT files.
    """
    lines = raw_text.split('\n')
    clean = []
    for line in lines:
        line = line.strip()
        # Filter timestamps (-->) and header junk
        if '-->' not in line and line and not line.isdigit() and line != 'WEBVTT':
            clean.append(line)
    # Remove duplicates (VTT often repeats lines) and join
    return " ".join(list(dict.fromkeys(clean)))

def analyze_video(transcript: str):
    if not client: return None
    prompt = f"""
    Classify into Roots: {json.dumps(list(THE_ATLAS.keys()))}
    Create 3 verification questions (A,B,C) based on this text.
    OUTPUT JSON: {{ "root_category": "...", "questions": [...] }}
    TEXT: {transcript[:15000]}
    """
    try:
        response = client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[{"role": "user", "content": prompt}],
            response_format={"type": "json_object"}
        )
        return json.loads(response.choices[0].message.content)
    except Exception:
        return None
